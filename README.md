# MindCube Dataset Framework

**Version 2.0** - Comprehensive spatial reasoning dataset generation and evaluation framework for vision-language models.

## ğŸŒŸ Overview

MindCube is a modular framework for generating and evaluating spatial reasoning datasets for multimodal AI models. The project follows a complete 5-step pipeline from raw data to model evaluation, with specialized modules for scaffold data curation, prompt generation, model inference, training, and comprehensive evaluation.

## ğŸ”„ Processing Pipeline

```
Raw Data â†’ Scaffold Data â†’ Model Prompts â†’ SFT Training â†’ Model Inference & Evaluation
    â†“           â†“              â†“             â†“                    â†“
  Step 1      Step 2        Step 3        Step 4              Step 5
 Input       Cogmap +      8 Task        Multi-Model         Performance
Processing   Reasoning     Variants      Training            Metrics
```

### Step 1: Raw Data Processing
- Original question-answer pairs with images
- Spatial relationship annotations (around, among, translation, rotation)
- Input format validation and preprocessing

### Step 2: Scaffold Data Generation
- **Cognitive Maps**: Scene understanding and object relationships
- **Reasoning Chains**: Step-by-step reasoning for spatial tasks
- **Full Pipeline**: Combined cognitive maps and reasoning (recommended)

### Step 3: Model Prompt Generation
- **8 Task Types**: Various input/output configurations for model training
- **Template-based Generation**: Consistent prompt formatting
- **Multi-modal Integration**: Text and visual input processing

### Step 4: SFT Training Data Generation
- **Model-Specific Formats**: Qwen2.5-VL, LLaVA, InstructBLIP support
- **Conversation Format**: Structured training data conversion
- **Extensible Architecture**: Easy to add new model formats

### Step 5: Model Operations & Evaluation
- **Multi-Model Inference**: Support for various VLM architectures
- **Comprehensive Evaluation**: Spatial reasoning metrics and cogmap evaluation
- **Performance Analysis**: Detailed metrics and error analysis

## ğŸ“ Project Structure

```
07_MindCube_new/
â”œâ”€â”€ src/                                # Core implementation modules
â”‚   â”œâ”€â”€ scaffold_curation/              # Step 2: Scaffold data generation
â”‚   â”‚   â”œâ”€â”€ processors.py               # Data processing pipeline
â”‚   â”‚   â”œâ”€â”€ formatters.py               # Output formatting utilities
â”‚   â”‚   â”œâ”€â”€ cogmap/                     # Cognitive map generation
â”‚   â”‚   â””â”€â”€ reasoning/                  # Reasoning chain generation
â”‚   â”‚
â”‚   â”œâ”€â”€ prompt_generation/              # Step 3: Model prompt generation
â”‚   â”‚   â”œâ”€â”€ processors.py               # Prompt processing logic
â”‚   â”‚   â”œâ”€â”€ generators.py               # Task-specific generators
â”‚   â”‚   â””â”€â”€ templates.py                # Prompt templates library
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                       # Step 4: Model training utilities
â”‚   â”œâ”€â”€ inference/                      # Model inference interfaces  
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                     # Step 5: Evaluation framework
â”‚   â”‚   â”œâ”€â”€ evaluator.py                # Main evaluation interface
â”‚   â”‚   â”œâ”€â”€ core/                       # Base evaluation metrics
â”‚   â”‚   â”œâ”€â”€ cogmap/                     # Cognitive map evaluation
â”‚   â”‚   â””â”€â”€ metrics/                    # Specialized metrics
â”‚   â”‚
â”‚   â””â”€â”€ utils/                          # Shared utilities
â”‚       â”œâ”€â”€ io_utils.py                 # File I/O operations
â”‚       â”œâ”€â”€ text_utils.py               # Text processing utilities
â”‚       â””â”€â”€ spatial_utils.py            # Spatial reasoning utilities
â”‚
â”œâ”€â”€ scripts/                            # User-friendly interfaces
â”‚   â”œâ”€â”€ data_processing.py              # Scaffold generation script
â”‚   â”œâ”€â”€ generate_prompts.py             # Prompt generation script
â”‚   â”œâ”€â”€ generate_reasoning.py           # Reasoning chain generation
â”‚   â”œâ”€â”€ convert_to_sft.py               # SFT data conversion script
â”‚   â”œâ”€â”€ run_inference.py                # Model inference script
â”‚   â”œâ”€â”€ run_evaluation.py               # Evaluation script
â”‚   â”œâ”€â”€ run_training.py                 # Training script
â”‚   â””â”€â”€ bash_scripts/                   # Batch processing scripts
â”‚
â”œâ”€â”€ data/                               # Data storage with organized structure
â”‚   â”œâ”€â”€ raw/                            # Original input data
â”‚   â”œâ”€â”€ scaffold/                       # Scaffold generation outputs
â”‚   â”‚   â””â”€â”€ all/                        # Full pipeline outputs (recommended)
â”‚   â”œâ”€â”€ prompts/                        # Generated prompts
â”‚   â”‚   â”œâ”€â”€ general/                    # All 8 prompt task types
â”‚   â”‚   â””â”€â”€ training/                   # SFT training data
â”‚   â”‚       â”œâ”€â”€ qwen2.5vl/              # Qwen SFT training data
â”‚   â”‚       â”œâ”€â”€ llava/                  # LLaVA SFT training data
â”‚   â”‚       â””â”€â”€ instructblip/           # InstructBLIP SFT training data
â”‚   â”œâ”€â”€ results/                        # Model inference results
â”‚   â””â”€â”€ tmp/                            # Temporary processing files
â”‚
â”œâ”€â”€ configs/                            # Configuration files
â”‚   â””â”€â”€ qwen_inference.json             # Model inference configurations
â”‚
â”œâ”€â”€ experiments/                        # Experiment tracking
â”‚   â””â”€â”€ sft/                            # SFT training experiments
â”‚
â”œâ”€â”€ notebooks/                          # Jupyter notebooks for analysis
â”œâ”€â”€ logs/                               # Processing and training logs
â”œâ”€â”€ checkpoints/                        # Model checkpoints
â””â”€â”€ Qwen2.5-VL/                        # Local model storage
```

## ğŸš€ Quick Start

### Environment Setup

```bash
# Create conda environment
conda create -n mindcube python=3.8
conda activate mindcube

# Install dependencies (create requirements.txt based on your needs)
# pip install -r requirements.txt

# Key dependencies typically include:
pip install torch torchvision transformers
pip install opencv-python pillow
pip install datasets accelerate
pip install vllm  # Optional: for faster inference
```

## ğŸ“‹ Core Usage

### ğŸ—ï¸ **Step 2: Scaffold Data Generation** (`data_processing.py`)

#### â­ **Full Pipeline Generation (Recommended)**
```bash
# Generate complete scaffold data (cognitive_map + reasoning_chain)
python scripts/data_processing.py \
  --input data/raw/MindCube_tinybench.jsonl \
  --task full_pipeline
python scripts/data_processing.py \
  --input data/raw/MindCube_train.jsonl \
  --task full_pipeline
# Output: ./data/scaffold/all/MindCube_tinybench.jsonl
# Contains: cognitive_map + reasoning_chain + all original fields
```

#### ğŸ”§ **Component-wise Generation**
```bash
# Generate cognitive maps only
python scripts/data_processing.py \
  --input data/raw/MindCube_tinybench.jsonl \
  --task cogmap

# Generate reasoning chains only
python scripts/data_processing.py \
  --input data/raw/MindCube_tinybench.jsonl \
  --task reasoning
```

---

### ğŸ’¬ **Step 3: Prompt Generation** (`generate_prompts.py`)

#### â­ **Generate All Task Types (Recommended)**
```bash
# Generate all 8 prompt task types from scaffold data
python scripts/generate_prompts.py \
  --input data/scaffold/all/MindCube_tinybench.jsonl \
  --all_tasks
python scripts/generate_prompts.py \
  --input data/scaffold/all/MindCube_train.jsonl \
  --all_tasks

# Output: ./data/prompts/general/
# Generates: MindCube_tinybench_{task_name}.jsonl (8 files)
```

#### ğŸ¯ **Single Task Type Generation**
```bash
# Generate specific task type
python scripts/generate_prompts.py \
  --input data/scaffold/all/MindCube_tinybench.jsonl \
  --task raw_qa \
  --output data/prompts/general/custom_output.jsonl

# List all available task types
python scripts/generate_prompts.py --list_tasks
```

---

### ğŸ‹ï¸ **Step 4: SFT Data Conversion** (`convert_to_sft.py`)

#### â­ **Batch Convert All Prompt Files**
```bash
# Convert all prompt files to Qwen2.5-VL SFT format
python scripts/convert_to_sft.py \
  --input_dir data/prompts/general/ \
  --model qwen2.5vl
# Output: ./data/prompts/training/qwen2.5vl/

# Convert to LLaVA format
python scripts/convert_to_sft.py \
  --input_dir data/prompts/general/ \
  --model llava
# Output: ./data/prompts/training/llava/

# List supported models
python scripts/convert_to_sft.py --list_models
```

#### ğŸ¯ **Single File Conversion**
```bash
# Convert specific task to Qwen format
python scripts/convert_to_sft.py \
  --input data/prompts/general/MindCube_tinybench_raw_qa.jsonl \
  --model qwen2.5vl \
  --output data/prompts/training/qwen2.5vl/custom_sft.json
```

---

### ğŸ¤– **Step 5: Model Inference** (`run_inference.py`)

#### â­ **Basic Qwen2.5-VL Inference**
```bash
# Use default HuggingFace model
python scripts/run_inference.py \
  --model-type qwen2.5vl \
  --input-file data/prompts/general/MindCube_tinybench_raw_qa.jsonl \
  --output-dir results/

# Output will be auto-generated as:
# results/MindCube_tinybench_raw_qa_qwen2.5-vl-3b-instruct_responses.jsonl
```

#### ğŸš€ **Accelerated Inference with vLLM**
```bash
# Use vLLM for faster inference
python scripts/run_inference.py \
  --model-type qwen2.5vl \
  --backend vllm \
  --input-file data/prompts/general/MindCube_tinybench_raw_qa.jsonl \
  --output-dir results/
```

#### âš™ï¸ **Fine-tuned Model Inference**
```bash
# Use your own fine-tuned checkpoint
python scripts/run_inference.py \
  --model-type qwen2.5vl \
  --model-path /path/to/your/checkpoint \
  --input-file data/prompts/general/MindCube_tinybench_raw_qa.jsonl \
  --output-file results/qwen_responses.jsonl
```

---

### ğŸ“Š **Step 6: Model Evaluation** (`run_evaluation.py`)

#### â­ **Comprehensive Evaluation**
```bash
# Evaluate model responses with multiple metrics
python scripts/run_evaluation.py \
  --predictions results/model_responses.jsonl \
  --ground_truth data/prompts/general/MindCube_tinybench_raw_qa.jsonl \
  --output-dir evaluation_results/

# Specify evaluation metrics
python scripts/run_evaluation.py \
  --predictions results/model_responses.jsonl \
  --ground_truth data/prompts/general/MindCube_tinybench_raw_qa.jsonl \
  --metrics accuracy f1_score cogmap_similarity \
  --output-dir evaluation_results/
```

---

## ğŸ”„ **Complete Workflow Example**

```bash
# Step 1: Generate scaffold data (one-time setup)
python scripts/data_processing.py \
  --input data/raw/MindCube_tinybench.jsonl \
  --task full_pipeline

# Step 2: Generate all prompt types (one-time setup)
python scripts/generate_prompts.py \
  --input data/scaffold/all/MindCube_tinybench.jsonl \
  --all_tasks

# Step 3: Convert to SFT training format (optional)
python scripts/convert_to_sft.py \
  --input_dir data/prompts/general/ \
  --model qwen2.5vl

# Step 4: Run model inference
python scripts/run_inference.py \
  --model-type qwen2.5vl \
  --input-file data/prompts/general/MindCube_tinybench_raw_qa.jsonl \
  --output-dir results/

# Step 5: Evaluate model performance
python scripts/run_evaluation.py \
  --predictions results/MindCube_tinybench_raw_qa_qwen2.5-vl-3b-instruct_responses.jsonl \
  --ground_truth data/prompts/general/MindCube_tinybench_raw_qa.jsonl \
  --output-dir evaluation_results/
```

## ğŸ“‹ **Available Task Types & Features**

### Scaffold Generation Tasks
- `cogmap` - Generate cognitive maps only
- `reasoning` - Generate reasoning chains only
- `full_pipeline` - Generate both (recommended)

### Prompt Generation Tasks (8 types)
- `raw_qa` - Basic question-answering without scaffolds
- `ff_rsn` - Free-form reasoning generation
- `aug_cgmap_in` - Augmented cognitive map as input
- `aug_cgmap_out` - Augmented cognitive map as output
- `plain_cgmap_out` - Plain cognitive map as output
- `plain_cgmap_ffr_out` - Plain cognitive map with first-few reasoning
- `aug_cgmap_ffr_out` - Augmented cognitive map with first-few reasoning
- `cgmap_in_ffr_out` - Cognitive map input with first-few reasoning output

### Supported SFT Training Models
- `qwen2.5vl` - Qwen2.5-VL conversation format with image placeholders
- `llava` - LLaVA training format with single image focus
- `instructblip` - InstructBLIP format with text input/output pairs

### Evaluation Metrics
- **Accuracy**: Basic answer correctness
- **F1 Score**: Precision and recall balance
- **Cognitive Map Similarity**: Semantic similarity of generated cognitive maps
- **Reasoning Quality**: Multi-step reasoning evaluation
- **Spatial Understanding**: Spatial relationship accuracy

## ğŸ“‹ **Data Format Specifications**

### Raw Input Data Format
```json
{
  "id": "unique_identifier",
  "category": "spatial",
  "type": "rotation|translation|among|around",
  "meta_info": {"setting": "indoor|outdoor"},
  "question": "Based on the images provided: [question text] A. Option1 B. Option2 ...",
  "images": ["path/to/image1.jpg", "path/to/image2.jpg"],
  "gt_answer": "A|B|C|D"
}
```

### Scaffold Data Format (After Step 2)
```json
{
  "id": "unique_identifier",
  "category": "spatial",
  "type": "rotation",
  "meta_info": {"setting": "indoor"},
  "question": "Based on the images provided: ...",
  "images": ["image1.jpg", "image2.jpg"],
  "gt_answer": "A",
  
  // Added by scaffold generation
  "cogmap": "The scene shows spatial relationships between...",
  "reasoning_chain": "Step-by-step reasoning process..."
}
```

### Model Prompt Format (After Step 3)
```json
{
  "id": "unique_identifier",
  "category": "spatial",
  "type": "rotation",
  "meta_info": {"setting": "indoor"},
  "question": "Based on the images provided: ...",
  "images": ["image1.jpg", "image2.jpg"],
  "gt_answer": "A",
  
  // Model-ready fields
  "input_prompt": "[Answer Format]\nBased on these images, answer the question...",
  "grounded_output": "Step-by-step reasoning leading to the final answer..."
}
```

### SFT Training Format Examples

**Qwen2.5-VL Format:**
```json
{
  "images": ["image1.jpg", "image2.jpg"],
  "conversations": [
    {
      "from": "human",
      "value": "<image>\n<image>\n[Task]\nAnalyze spatial relationships..."
    },
    {
      "from": "gpt",
      "value": "<think>Step-by-step reasoning...</think><answer>A. Above</answer>"
    }
  ]
}
```

## ğŸ› ï¸ **Advanced Usage**

### Batch Processing
```bash
# Process all files in a directory
python scripts/data_processing.py \
  --batch_dir input_dir/ \
  --output_dir output_dir/ \
  --task full_pipeline
```

### Validation and Preview
```bash
# Validate scaffold data
python scripts/generate_prompts.py \
  --input scaffold.jsonl \
  --validate

# Preview generated prompts
python scripts/generate_prompts.py \
  --input scaffold.jsonl \
  --preview \
  --samples 3
```

### Custom Configuration
```bash
# Use custom inference configuration
python scripts/run_inference.py \
  --model-type qwen2.5vl \
  --config configs/custom_inference.json \
  --input-file prompts.jsonl \
  --output-dir results/
```

### Help and Documentation
```bash
# Get help for any script
python scripts/data_processing.py --help
python scripts/generate_prompts.py --help
python scripts/run_inference.py --help
python scripts/run_evaluation.py --help

# List available options
python scripts/generate_prompts.py --list_tasks
python scripts/convert_to_sft.py --list_models
python scripts/run_inference.py --list-models
```

## ğŸ”¬ **Research & Development**

### Experiment Tracking
The framework supports structured experiment tracking through the `experiments/` directory:
- SFT training experiments in `experiments/sft/`
- Model checkpoints in `checkpoints/`
- Processing logs in `logs/`

### Jupyter Notebooks
Use the `notebooks/` directory for:
- Data analysis and visualization
- Model performance analysis
- Prototype development
- Custom metric development

### Extending the Framework
The modular architecture makes it easy to:
- Add new prompt task types in `src/prompt_generation/templates.py`
- Implement new evaluation metrics in `src/evaluation/metrics/`
- Support new model formats in SFT conversion
- Add custom processing steps in the scaffold curation pipeline

## ğŸ¤ **Contributing**

To contribute to MindCube:

1. Fork the repository
2. Create a feature branch
3. Follow the existing code structure and English comment convention
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“ **License**

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“§ **Contact**

For questions, issues, or collaboration opportunities, please:
- Open an issue on GitHub
- Contact the development team
- Join our research discussions

---

**MindCube Framework** - Advancing spatial reasoning capabilities in vision-language models through comprehensive dataset generation and evaluation.

# MindCube vLLMåŠ é€Ÿæ¨ç†

ä¸“æ³¨äºä½¿ç”¨vLLMåŠ é€ŸQwen2.5-VLå¤šå›¾æ¨ç†ï¼Œæ”¯æŒæœ€å¤š4å¼ å›¾ç‰‡è¾“å…¥ï¼Œè¾“å‡ºé™åˆ¶1536 tokensã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# 1. å®‰è£…vLLM (éœ€è¦CUDA 12.1+)
pip install vllm>=0.7.2

# 2. å®‰è£…Qwen2.5-VLä¾èµ–
pip install transformers>=4.51.3 accelerate qwen-vl-utils[decord]

# 3. éªŒè¯ç¯å¢ƒ
python test_vllm_specific.py
```

### 2. vLLMæ¨ç†é…ç½®

å½“å‰é…ç½®ï¼š`configs/vllm_optimized.json`

```json
{
  "model_type": "qwen2.5vl",
  "backend": "vllm",
  "generation_config": {
    "max_new_tokens": 1536,
    "temperature": 0.0
  },
  "vllm_config": {
    "gpu_memory_utilization": 0.95,
    "max_model_len": 32768,
    "limit_mm_per_prompt": {
      "image": 4,
      "video": 1
    },
    "trust_remote_code": true,
    "dtype": "bfloat16",
    "enable_prefix_caching": true,
    "enable_chunked_prefill": true,
    "max_num_seqs": 32,
    "max_num_batched_tokens": 4096
  }
}
```

### 3. ä½¿ç”¨æ–¹æ³•

#### å•æ¬¡æ¨ç†
```bash
python scripts/run_inference.py --backend vllm --config configs/vllm_optimized.json
```

#### æ‰¹é‡æ¨ç†
```bash
python scripts/run_inference.py \
  --backend vllm \
  --batch-size 16 \
  --config configs/vllm_optimized.json \
  --input-file data/prompts/general/MindCube_tinybench_raw_qa.jsonl \
  --output-dir data/outputs/
```

#### å¤šå›¾æ¨ç†ç¤ºä¾‹

```python
from vllm import LLM, SamplingParams
from PIL import Image

# åˆå§‹åŒ–vLLM
llm = LLM(
    model="Qwen/Qwen2.5-VL-3B-Instruct",
    gpu_memory_utilization=0.95,
    max_model_len=32768,
    trust_remote_code=True,
    limit_mm_per_prompt={"image": 4}
)

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.0,
    max_tokens=1536
)

# å¤šå›¾æ¨ç†
images = [Image.open(f"image_{i}.jpg") for i in range(4)]
prompt = "è¯·åˆ†æè¿™4å¼ å›¾ç‰‡çš„å…±åŒç‰¹å¾ã€‚"

outputs = llm.generate(
    {
        "prompt": prompt,
        "multi_modal_data": {"image": images}
    },
    sampling_params=sampling_params
)

print(outputs[0].outputs[0].text)
```

## ğŸ¯ å…³é”®ç‰¹æ€§

- âœ… **vLLMåŠ é€Ÿ**ï¼šä½¿ç”¨vLLMå¼•æ“è·å¾—æœ€ä½³æ¨ç†æ€§èƒ½
- ğŸ–¼ï¸ **å¤šå›¾æ”¯æŒ**ï¼šæ”¯æŒæœ€å¤š4å¼ å›¾ç‰‡åŒæ—¶è¾“å…¥
- ğŸš€ **é«˜ååé‡**ï¼šä¼˜åŒ–é…ç½®æ”¯æŒå¤§æ‰¹é‡å¹¶å‘æ¨ç†
- ğŸ’¾ **æ™ºèƒ½ç¼“å­˜**ï¼šå¯ç”¨å‰ç¼€ç¼“å­˜å’Œåˆ†å—é¢„å¡«å……
- âš¡ **95% GPUåˆ©ç”¨ç‡**ï¼šä¸å…³å¿ƒå†…å­˜ä½¿ç”¨ï¼Œä¸“æ³¨æœ€å¤§æ€§èƒ½

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### GPUé…ç½®
- **å†…å­˜ä½¿ç”¨**ï¼š95% GPUå†…å­˜åˆ©ç”¨ç‡
- **åºåˆ—é•¿åº¦**ï¼šæ”¯æŒ32Kä¸Šä¸‹æ–‡é•¿åº¦
- **æ‰¹é‡å¤§å°**ï¼šæ¨è16-32ï¼Œæ ¹æ®GPUå†…å­˜è°ƒæ•´
- **ç²¾åº¦**ï¼šbfloat16ç”¨äºæœ€ä½³æ€§èƒ½

### å¤šæ¨¡æ€é…ç½®
- **å›¾ç‰‡é™åˆ¶**ï¼šæœ€å¤š4å¼ å›¾ç‰‡/prompt
- **è¾“å‡ºé™åˆ¶**ï¼š1536 tokensæœ€å¤§è¾“å‡º
- **è§†é¢‘æ”¯æŒ**ï¼šæ”¯æŒä½†é™åˆ¶1ä¸ªè§†é¢‘/prompt

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **GPUå†…å­˜ä¸è¶³**
   ```bash
   # é™ä½å†…å­˜ä½¿ç”¨
   "gpu_memory_utilization": 0.8
   ```

2. **vLLMç‰ˆæœ¬é”™è¯¯**
   ```bash
   pip install vllm>=0.7.2
   ```

3. **æ¨¡å‹åŠ è½½å¤±è´¥**
   ```bash
   # ç¡®ä¿trust_remote_code=true
   "trust_remote_code": true
   ```

### æ€§èƒ½è°ƒä¼˜

1. **å¢åŠ æ‰¹é‡å¤§å°**ï¼š
   - ä¿®æ”¹`max_num_seqs`å’Œ`batch_size`

2. **ä¼˜åŒ–å†…å­˜ä½¿ç”¨**ï¼š
   - è°ƒæ•´`max_model_len`å’Œ`max_num_batched_tokens`

3. **å¤šGPUæ”¯æŒ**ï¼š
   - è®¾ç½®`tensor_parallel_size`

## ğŸ“ é¡¹ç›®ç»“æ„

```
07_MindCube_new/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ vllm_optimized.json     # vLLMä¼˜åŒ–é…ç½®
â”œâ”€â”€ test_vllm_specific.py       # vLLMæµ‹è¯•è„šæœ¬
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_inference.py        # ä¸»æ¨ç†è„šæœ¬
â””â”€â”€ src/
    â””â”€â”€ inference/               # æ¨ç†å¼•æ“ä»£ç 
```

## ğŸ¨ ä½¿ç”¨æ¡ˆä¾‹

### å¤šå›¾æ¯”è¾ƒåˆ†æ
```python
# æ¯”è¾ƒå¤šå¼ å›¾ç‰‡çš„å¼‚åŒ
prompt = "è¯·æ¯”è¾ƒè¿™å‡ å¼ å›¾ç‰‡çš„å¼‚åŒç‚¹ï¼Œå¹¶ç»™å‡ºè¯¦ç»†åˆ†æã€‚"
images = [img1, img2, img3, img4]
```

### æ‰¹é‡æ–‡æ¡£ç†è§£
```python
# æ‰¹é‡å¤„ç†æ–‡æ¡£å›¾ç‰‡
prompts = ["æ€»ç»“è¿™ä»½æ–‡æ¡£çš„è¦ç‚¹ã€‚" for _ in range(batch_size)]
image_batches = [doc_images for doc_images in document_batches]
```

### è§†è§‰é—®ç­”
```python
# åŸºäºå›¾ç‰‡å†…å®¹å›ç­”é—®é¢˜
prompt = "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆç‰©ä½“ï¼Ÿå®ƒä»¬çš„ä½ç½®å…³ç³»å¦‚ä½•ï¼Ÿ"
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ä»…æ”¯æŒvLLMåç«¯**ï¼šæœ¬é…ç½®ä¸“é—¨ä¸ºvLLMä¼˜åŒ–
2. **é«˜å†…å­˜ä½¿ç”¨**ï¼š95% GPUå†…å­˜åˆ©ç”¨ç‡éœ€è¦è¶³å¤Ÿçš„æ˜¾å­˜
3. **è¾“å‡ºé™åˆ¶**ï¼šä¸¥æ ¼é™åˆ¶1536 tokensè¾“å‡º
4. **å›¾ç‰‡æ•°é‡**ï¼šæœ€å¤š4å¼ å›¾ç‰‡/prompt

---

ğŸš€ **ç«‹å³å¼€å§‹vLLMåŠ é€Ÿæ¨ç†ï¼** 